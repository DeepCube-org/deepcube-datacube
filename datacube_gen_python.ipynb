{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2c57f1-8a3a-4d29-ae73-e1f0df38e369",
   "metadata": {},
   "source": [
    "# Document cube generation...\n",
    "\n",
    "Documentation on data cubes for DeepCube users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000674f-a6d8-41f8-8350-26eb7c6c1d10",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Need a working python3 installation.\n",
    "\n",
    "If not install Miniconda3 and from there python3.\n",
    "\n",
    "Create environment with e.g. python3\n",
    "\n",
    "Install the following packages: xarray, netCDF4, scipy, zarr, rasterio, dask\n",
    "\n",
    "Need a working gdal installation to repoject data if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0441b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate conda environment\n",
    "%conda activate myenv\n",
    "# install python packages\n",
    "%conda install xarray netCDF4 scipy zarr rasterio dask fsspec --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a096052-50d9-40e4-a49a-77bb1df228d2",
   "metadata": {},
   "source": [
    "## Create scratch directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355593d2-3c35-4ee6-8949-d053989c15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "myscratch = \"/Net/Groups/BGI/scratch/mweynants\"\n",
    "if (not(os.path.isdir(myscratch))) :\n",
    "    os.mkdir(myscratch)\n",
    "# set myscratch as current directory\n",
    "os.chdir(myscratch)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd3e34-81c6-409f-9322-e64a75fda72d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create cube with xarray {#minicube}\n",
    "For **small cubes**, one can directly use `xarray` (https://xarray.pydata.org/en/stable/getting-started-guide/installing.html).\n",
    "\n",
    "This is a minimal example on how to build data cubes using `xarray` only. It is assumed that all inputs are already transformed to a EPSG:4326 longitude-latitude grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5435e-874f-4969-9574-4bc303720707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "import numpy as np\n",
    "\n",
    "era = xr.open_dataset(\"/Net/Groups/BGI/scratch/fgans/uc3-mini-dataset/ERA5-LAND/era5-land-hourly.nc\")\n",
    "clc = xr.open_dataset(\"/Net/Groups/BGI/scratch/fgans/uc3-mini-dataset/CLC-2018-CROPPED_NC/cropped_CLC_2018.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498497d-a236-46c8-8c80-9874365c93d9",
   "metadata": {},
   "source": [
    "The next step would be to decide for a target spatial resolution and to create regridded representations of all input datasets. Here we assume that the target would be the CLC resolution. Then we can use xarrays interp method to interpolate the data. The interpolation will fall back to scipy's interpolation methods and allows linearandnearest interpolations. For more complex/individual regridding, one can use gdal (see previous step) or xcube (next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3222b197-934f-4006-bfae-2c4a8f63d06b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2a9ea-5881-4820-a360-219bf1068793",
   "metadata": {},
   "outputs": [],
   "source": [
    "era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3506f6-7064-425c-b37c-442979a92167",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_lon = clc.lon\n",
    "target_lat = clc.lat\n",
    "# convert era into dask arrays with a single block and interpolate along lon and lat\n",
    "era_interp = era.chunk().interp({'longitude':target_lon, 'latitude':target_lat},  method = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b5adb-1e0f-4d52-9afc-0ca098200d97",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "era_interp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f9aee-8eeb-4da7-b0f7-8dc0ab837eb7",
   "metadata": {},
   "source": [
    "Please note that it is **essential to call chunk on the dataset to be regridded first to convert it to a dask array which is evaluated lazily**.\n",
    "\n",
    "[xarray.DataArray.chunk](https://xarray.pydata.org/en/stable/generated/xarray.DataArray.chunk.html)\n",
    "\n",
    "[xarray.Dataset.to_zarr](https://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_zarr.html)\n",
    "\n",
    "In the next step we would collect all new output variables into one dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb80d0b-b3dd-40bf-a7bb-b04ba6be213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vars = {}\n",
    "for k in era_interp.keys():\n",
    "    output_vars[k] = era_interp[k]\n",
    "output_vars['clc'] = clc['Band1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1efbb-deb9-4c2a-bd1d-25969f042220",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3f5a0-0e96-4ebb-a7e2-735e7dfe3427",
   "metadata": {},
   "source": [
    "Then we create a new dataset from these DataArrays, select a target chunking and write the result to a new zarr file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2ea55-a9b2-402b-98a3-ba1242456c0e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = xr.Dataset(output_vars).chunk({'lon':200, 'lat':200}).to_zarr(\"./mydatacubelinear.zarr\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84524b06-4de8-4d55-a1a0-a36c3f4bced5",
   "metadata": {},
   "source": [
    "Note that this new dataset has identical spatial resolutions and through the small spatial chunks will have fast access to individual time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46085cd3-8346-40e8-993b-3d66035d4e36",
   "metadata": {},
   "source": [
    "## Data Cube generation using xcube-gen{#xcube}\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- every dataset is in NetCDF format, can consist of multiple files, but should be in a single folder\n",
    "- datasets should already be in equirectangular lon-lat coordinate system\n",
    "- make a config file in yaml format, an example config file would be:\n",
    "\n",
    "### Prepare yaml file\n",
    "Save configuration file as xcube/config.yaml. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be81db",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "output_size: [2000,2000]\n",
    "output_region: [20, 36, 24, 39]\n",
    "output_writer_name: 'zarr'\n",
    "\n",
    "output_metadata:\n",
    "  created_by: 'NOA'\n",
    "  contact_email: 'yourmail@example.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350ba8f",
   "metadata": {},
   "source": [
    "\n",
    "- see [xcube-gen docs](https://xcube.readthedocs.io/en/latest/cli/xcube_gen.html) for more options\n",
    "- make sure that all static variables have a time dimension (use `ds.expand_dims`)\n",
    "- and add `time_bnds` attribute to them so they can be processed by the xcube default processor\n",
    "\n",
    "## Run xcube gen\n",
    "Cube the data with :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81e124",
   "metadata": {},
   "source": [
    "\n",
    "````\n",
    "xcube gen -c xcube/config.yaml -o ./xcc.zarr ${PATH1}/*.nc ${PATH2}/cropped_CLC_2018_time.nc\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c479e9",
   "metadata": {},
   "source": [
    "## Rechunking{#rechunk}\n",
    "If the number of chunks is too large, the xarray function to_zarr will fail to write the dask array to a zarr group.\n",
    "\n",
    "### Rechunker{#rechunker}\n",
    "Look into Python package [Rechunker](https://rechunker.readthedocs.io/en/latest/tutorial.html) to re-write Zarr with different chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ebc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install rechunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54328b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rechunker import rechunk\n",
    "import xarray as xr\n",
    "xr.set_options(display_style='text')\n",
    "import zarr\n",
    "import dask.array as dsa\n",
    "\n",
    "ds = xr.tutorial.open_dataset(\"air_temperature\")\n",
    "# create initial chunk structure\n",
    "ds = ds.chunk({'time': 100})\n",
    "ds.air.encoding = {} # helps when writing to zarr\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3935a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up any existing temporary data\n",
    "! rm -rf *.zarr \n",
    "# write to zarr\n",
    "ds.to_zarr('air_temperature.zarr')\n",
    "# open zarr group\n",
    "source_group = zarr.open('air_temperature.zarr')\n",
    "print(source_group.tree())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at one array\n",
    "source_array = source_group['air']\n",
    "source_array.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6610198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rechunk Group\n",
    "target_chunks = {\n",
    "    'air': {'time': 2920, 'lat': 25, 'lon': 1},\n",
    "    'time': None, # don't rechunk this array\n",
    "    'lon': None,\n",
    "    'lat': None,\n",
    "}\n",
    "max_mem = '1MB'\n",
    "\n",
    "target_store = 'group_rechunked.zarr'\n",
    "# define temporate storage\n",
    "temp_store = 'group_rechunked-tmp.zarr'\n",
    "\n",
    "# need to remove the existing stores or it won't work\n",
    "!rm -rf group_rechunked.zarr group_rechunked-tmp.zarr\n",
    "# prepare empty output zarr\n",
    "array_plan = rechunk(source_group, target_chunks, max_mem, target_store, temp_store=temp_store)\n",
    "array_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8dec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute plan\n",
    "array_plan.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c79f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_zarr('group_rechunked.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229509ea",
   "metadata": {},
   "source": [
    "### Rechunking in Julia{#jlrechunk}\n",
    "\n",
    "Dask often fails to rechunk large cubes. We provide code in Julia for that purpose. See example in [datacube_rechunk.jl](./datacube_rechunk_example.jl)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b33191-2017-43c2-9edd-05a04b485013",
   "metadata": {},
   "source": [
    "### NCO ncap2{#ncrechunk}\n",
    "Rechunk the netcdf with the [netCDF Operator (NCO)](http://nco.sourceforge.net/nco.html) [ncap2](http://nco.sourceforge.net/nco.html#ncap2-netCDF-Arithmetic-Processor). See [Documentation on Chunking](http://nco.sourceforge.net/nco.html#Chunking) for details on usage.\n",
    "\n",
    "NCO can be installed with Anaconda (doesn't seem to be working for me on macadamia in myenv_python39:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692ac7c-02cb-4154-8684-f9dd1dfac155",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda install -c conda-forge nco # Linux or MacOS with Anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25949ac9-4715-4ebf-b3c1-3fd2681fec49",
   "metadata": {},
   "source": [
    "For example, a netCDF cube with fire data is re-chunked so that the chunk size of the time dimension is 1 (i.e. 1 time step by chunk), while x and y have 983 and 1253 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a190028-4b4c-425a-a378-e26277005c81",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ncap2 --cnk_dmn time,1 --cnk_dmn x,983 --cnk_dmn y,1253 ~/FireCube.nc  ~/FireCube_time1_x983_y1253.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d479f-0ba4-4528-a270-583b6797ad04",
   "metadata": {},
   "source": [
    "In python, write an empty zarr file with the correct metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd882c4-4a34-40c0-9899-d6da643262b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import zarr\n",
    "import dask.array as dsa\n",
    "\n",
    "zarr_path = \"myzarr.zarr\"\n",
    "ds.chunk({'time': 1}).to_zarr(zarr_path, compute=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb93ff4-3fe8-46ac-8fd7-99525b952556",
   "metadata": {},
   "source": [
    "Then, write the data with xarray in pieces of 100 chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456af020-8f12-4ec7-912d-ac3f2234868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 100\n",
    "for i in range (0, len(ds['time']), step):\n",
    "      start, stop = (i, i+step)\n",
    "      new_ds.isel(time=slice(start, stop)).to_zarr(zarr_path, \n",
    "region={\"time\": slice(start, stop)})\n",
    "      print(f'Written time ({start}-{stop})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b15d5-99e4-40a5-ad1c-ae0c1233dac3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create and deploy cube with panGeoForge recipes\n",
    "\n",
    "see doc in https://pangeo-forge.readthedocs.io/\n",
    "\n",
    "Install panGeoForge recipes python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae19ca-9967-4754-b73d-49616eee5ce1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda install -c conda-forge pangeo-forge-recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93378fc4-cd4a-4f86-9f38-7e4ca8a15eba",
   "metadata": {},
   "source": [
    "TO DO: look into XarrayZarrRecipe\n",
    "see how we can start from there\n",
    "https://pangeo-forge.readthedocs.io/en/latest/pangeo_forge_recipes/recipe_user_guide/recipes.html\n",
    "\n",
    "A Recipe is a Python object, which encapsulates a workflow for transforming data.\n",
    "It takes a FilePattern (describing a collection of source files) and turns it into a single analysis-ready, cloud-optimized dataset.\n",
    "The recipe can then be executed.\n",
    "\n",
    "The idea is to develop recipes for specific data format of source and target.\n",
    "Source needs to be defined clearly... Data format and file pattern.\n",
    "\n",
    "So I will need to have recipes that get \n",
    "from source :\n",
    "- gridded netCDF data?\n",
    "to target:\n",
    "- ovh store\n",
    "- object store on some DIAS (once I get the info from GAEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644e511-3c75-404c-84e4-ce282420f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_forge_recipes.recipes import XarrayZarrRecipe\n",
    "XarrayZarrRecipe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b51d5c-fcb6-4d42-b0ca-8ec4d0894e63",
   "metadata": {},
   "source": [
    "The [pangeo_forge_recipes.recipes.XarrayZarrRecipe](https://pangeo-forge.readthedocs.io/en/latest/pangeo_forge_recipes/api_reference.html#pangeo_forge_recipes.recipes.XarrayZarrRecipe) recipe class uses [Xarray](http://xarray.pydata.org/) to read the input files and [Zarr](https://zarr.readthedocs.io/) as the target dataset format. The inputs can be in [any file format Xarray can read](http://xarray.pydata.org/en/latest/user-guide/io.html), including NetCDF, OPeNDAP, GRIB, Zarr, and, via [rasterio](https://rasterio.readthedocs.io/), GeoTIFF and other geospatial raster formats. The target Zarr dataset can be written to any storage location supported by [filesystem-spec](https://filesystem-spec.readthedocs.io/); see [Storage](https://pangeo-forge.readthedocs.io/en/latest/pangeo_forge_recipes/recipe_user_guide/storage.html) for more details. The target Zarr dataset will conform to the [Xarray Zarr encoding conventions](http://xarray.pydata.org/en/latest/internals/zarr-encoding-spec.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8bb7a-8259-4e35-aa27-d266ce83988d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Define FilePattern\n",
    "\n",
    "# 2. Define target storage (+ cache [and metadata] storage)\n",
    "#  from pangeo_forge_recipes.storage import CacheFSSpecTarget, FSSpecTarget, MetadataTarget, StorageConfig\n",
    "\n",
    "# define your fsspec filesystems for the target, cache, and metadata locations here\n",
    "\n",
    "target = FSSpecTarget(fs=<fsspec-filesystem-for-target>, root_path=\"<path-for-target>\")\n",
    "cache = CacheFSSpecTarget(fs=<fsspec-filesystem-for-cache>, root_path=\"<path-for-cache>\")\n",
    "metadata = MetadataTarget(fs=<fsspec-filesystem-for-metadata>, root_path=\"<path-for-metadata>\")\n",
    "\n",
    "storage_config = StorageConfig(target, cache, metadata)\n",
    "\n",
    "# 3. Call recipe\n",
    "recipe = XarrayZarrRecipe(\n",
    "    file_pattern = FilePattern,\n",
    "    inputs_per_chunk = int,\n",
    "    target_chunks = Dict[str,int],\n",
    "    storge_config = storage_config\n",
    ")\n",
    "\n",
    "## Execution : different options\n",
    "### A. manual execution (will probably be dropped)\n",
    "# 1. Cache chunks\n",
    "for input_key in recipe.iter_inputs():\n",
    "    recipe.cache_input(input_key)\n",
    "# 2. Prepare target : prepares zarr group on target storage with coords (no data vars yet)\n",
    "recipe.prepare_target()\n",
    "# 3. Write chunks to target\n",
    "for chunk_key in recipe.iter_chunks():\n",
    "    recipe.store_chunk(chunk_key)\n",
    "#4. Finalise (cleanup and consolidation)\n",
    "recipe.finalize_target()\n",
    "\n",
    "### B. create python function\n",
    "recipe_func = recipe.to_function()\n",
    "recipe_func() # actually executes the recipe\n",
    "\n",
    "### C. Dask Delayed\n",
    "delayed = recipe.to_dask()\n",
    "delayed.compute()\n",
    "# can be executed by any of Dask's schedules, incl.cloud and HPC distributed schedulers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc-showcode": false,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
